\documentclass{article}
\usepackage[a4paper, tmargin=1in, bmargin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}

% \usepackage{parskip}
\usepackage{amsmath}
\usepackage{siunitx}
\sisetup{round-mode=places, round-precision=4}
\usepackage{ bbold }

\usepackage{pdflscape}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\title{EE 771 : Recent Topics in Analytical Signal Processing Paper Review 1}
\author{Arka Sadhu - 140070011}
\date{\today}

\begin{document}
\maketitle

\section*{Q1}
The paper has quite a few differences from the previous literature in the field. In particular:
\begin{itemize}
\item Earlier works use some or the other kind of fitness score and compare pairwise nodes and set a weight corresponding to the edge joining the two nodes. The fitness score in some sense evaluates the smoothness of the graph signal.
\item The fitness score can either be from a regression model, or correlations between wavelet coefficients or PCA applied to matrices.
\item The above class of algorithms only consider the local structure of the graph (pairwise correlation) and not the global structure. Also the paper uses factor analysis which can reveal a simple linear statistical model between the graph signal and the latent variables.
\item Another class of algorithms tackle a similar problem of multiple kernel learning. But they use some priors to construct the initial graph and then use smoothness constraint to refine the graph. However, this paper makes no such assumptions and the graph is learnt only through the signal observations.
\item The problem of learning graphical models from observed data and inferring graph structure for gaussian markov random fields are similar problems. However, it is known that there is a one-to-one correspondence between the algorithms for solving the above to partial correlations of random variables and hence it again collapses to the case of learning pairwise edge weights.
\item Also, the learned graphical model (inverse correlation matrix) cannot be easily projected to get a laplacian which has reuqirements like having row sum as 0, non-positive diagonal entries. Therefore learning the inverse correlation matrix doesn't reveal the global structure of the graph.
\item This paper, tries to jointly learn the link between the global smoothness and the graph topology using optimization techniques and this joint learning is what makes the work done novel.
\end{itemize}

\section*{Q2}
The smoothness of the graph signal is captured in equation (4) of the paper.
\begin{itemize}
\item The paper uses a factor analysis model and uses the following representation:
  $$ x = \chi h + u_x + \epsilon$$
\item Here $x \in \mathbb{R}^n$ is the observed graph signal, $h \in \mathbb{R}^n$ represents the latent variable which controls the graph signal $x$ through the eigen-vector matrix $\chi$. $u_x \in \mathbb{R}^n$ is the mean of $x$.
\item Further it is assumed that the latent variable $h$ has the following distribution:
  $$h \sim \mathcal{N}(0, \Lambda^{\dagger})$$
\item Here $\Lambda^{\dagger}$ is the moore-penrose inverse of the eigen-value matrix $\Lambda$.
\item Clearly, $h$ will have a higher probability of taking a lower value. $x$ is proportional to $\chi h$. $\chi$ has the usual fourier transform interpretation. Hence smaller values of $h$ implies the graph signal $x$ has more lower frequency components. And thus the smoothness of the graph signal is captured. Also higher the eigen-value, smaller is the standard deviation corresponding to that component and therefore more condensed towards zero. In other words, the higher frequency components have very small contribution.
\item We note that any distribution of $h$ which has condensed probability mass near zero is a suitable candidate and can be readily replaced with the gaussian distribution and still capture the smoothness.
\end{itemize}

\section*{Q3}

\end{document}
