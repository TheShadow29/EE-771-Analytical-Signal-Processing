\documentclass{article}
\usepackage[a4paper, tmargin=1in, bmargin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}

% \usepackage{parskip}
\usepackage{amsmath}
\usepackage{siunitx}
\sisetup{round-mode=places, round-precision=4}
\usepackage{ bbold }

\usepackage{pdflscape}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\title{EE 771 : Recent Topics in Analytical Signal Processing Assignment 2}
\author{Arka Sadhu - 140070011}
\date{\today}

\begin{document}
\maketitle

\section*{Q1}
We are given the bandlimited field $g(x, y)$ as
$$g(x,y) = \sum_{l=-10}^{10}\sum_{k=-5}^{5} a[k, l] exp(j2\pi kx + j2\pi ly)$$
We are also given that we move along the path $y = \sqrt{2}x$ and this path is denoted by L. $g$ is parametrized by time as
\begin{align}
  \label{eq:1}
  h(t) = g(t, \sqrt(2)t) \tag*{$0 \le t \le 1/\sqrt{2}$}
\end{align}

\subsection*{1a}
We need to find the degrees of freedom along the two axes of the 2d field $g$. First we consider along the x-axis i.e. y is a constant (here taken to be $y_0$) and x is variable.

$$g(x, y_0) = \sum_{l=-10}^{10} \sum_{k=-5}^5a[k, l] exp(j2 \pi kx + j2\pi ly_0)$$
$$g(x, y_0) = \sum_{k=-5}^{5} exp(j2 \pi kx) \sum_{l=-10}^{10} a[k, l] exp(j2\pi ly_0)$$
Let $\alpha [k] = \sum_{l=-10}^{10}a[k, l] exp(j2\pi ly_0)$
\begin{equation}
  \label{eq:gy0}
  g(x, y_0) = \sum_{k=-5}^{5} exp(j2 \pi kx) \alpha[k]
\end{equation}

Therefore we can reconstruct $g(x, y_0)$ from the values of $\alpha[k]$ and 11 such values are required. Therefore degrees of freedom along x-axis is 11.

For y-axis we have constant x (say $x_0$) and variable y.
$$\beta[l] = \sum_{k=-5}^5 a[k,l] exp(j2\pi kx_0)$$
\begin{equation}
  \label{eq:gx0}
  g(x_0, y) = \sum_{l=-10}^{10}exp(j2 \pi ly) \beta[l]
\end{equation}

Again, we can reconstruct $g(x_0, y)$ from the values of $\beta[l]$ and 21 such values are required. Therefore degrees of freedom along y-axis is 21.

\subsection*{1b}
We note that the representations in \ref{eq:gy0} and \ref{eq:gx0} readily suggest the bandlimitness of the 1d representation. For the case of $g(x, 0.5)$ we note that \ref{eq:gy0} directly implies that the maximum frequency requried would be when $k = +5$ or $k = -5$ both of which correspond to 5Hz. Also we note that this value is independent of the value of $y_0$. Similarly for the case of $g(0.25, y)$ we have maximum frequency of 10Hz. Therefore both 1d representations are bandlimited.

\subsection*{1c}
% We answer both the questions in 1c and 1d together. In particular, we show 1d to be true, i.e. we show how to reconstruct $g$ from $h$ which in itself proves our claim in 1c that $h$ indeed captures all the degrees of freedom in $g$.

We first note the formula for $h(t) = g(t, \sqrt{2}t)$ can be given as:
\begin{align}
  \label{eq:2}
  h(t) = \sum_{k=-5}^{k=5}\sum_{l=-10}^{l=10}a[k, l] exp(j2\pi kt + j2\pi l\sqrt{2}t) \tag*{$0 \le t \le 1 / \sqrt{2}$}
\end{align}

We further note that we can reconstruct $g(x, y)$ if we are given $a[k, l]$ for all $k = \{-5, -4 ... 5\}$ and $l = \{-10, -9 ... 10\}$. We also note that none of the complex exponentials overlap with the other, that is all the complex exponentials are distinct from each other. This is because the factor with $l$ is an irrational number and therefore the sampled frequency are all distinct from each other. Moreover, we know that the complex exponentials form an orthogonal basis and therefore each of the $a[k, l]$ terms can be recovered.

This indeed proves that $h$ captures all the degrees of freedom of $g$.

\subsection*{1d}
As argued in 1c, $h$ captures all the degrees of freedom in $g$. To recover $g$ from $h$ we need to extract out the values of $a[k, l]$. We note that we have access to the complete 1d signal $h(t)$.

Further using orthogonality property of complex exponentials we note:
$$f(t) = \sum_{n=-\infty}^{\infty} c_n exp(j2\pi n t)$$
$$c_n = \int_{-\frac{1}{2}}^{\frac{1}{2}} f(t)exp(-j 2\pi n t)dt$$

To recover $g$ from $h$ we do the following. For each $k = \{-5, -4 ... 5\}$ and $l = \{-10, -9 ... 10\}$ we get
$$a[k, l] = \int_{-\frac{1}{2}}^{\frac{1}{2}} h(t) exp(-j 2\pi t (k + l \sqrt{2}) dt$$.

Once we have $a[k, l]$, we can trivially find $g(x,y)$ using its definition.


\section*{Q2}
We are given that $W(x)$ is a stationary noise process. We are also given that $S_W(\omega) \propto \frac{1}{\omega}$ for $|\omega| \ge \frac{\pi}{X}$ where $X$ is the sampling distance.

From power spectral density theory we know that:
\begin{align}
  \label{eq:psd1}
  S_{W_s}(\omega) = \frac{1}{X}\sum_{k=-\infty}^{\infty}S_W(\omega - \frac{2\pi k}{X}) \tag*{$|\omega| \le \frac{\pi}{X}$}
\end{align}
We note that the required variance of $W_S$ is given by $\sigma^2 = \frac{1}{\pi} \int_{0}^{\infty}S_{W_s}(\omega)d\omega$
$$S_{W_s}(\omega) = \frac{1}{X} \sum_{k=-\infty}^{\infty}S_W(\omega - \frac{2\pi k}{X})$$
$$S_{W_s}(\omega) = \frac{S_W(\omega)}{X} + \frac{1}{X} \sum_{k \in \mathcal{Z}, k \ne 0} S_W(\omega - \frac{2 \pi k}{X})$$

Next we note that $\hat{\omega} = |\omega - \frac{2\pi k}{X}| \ge \frac{\pi}{X}$ for all $|\omega| \le \frac{\pi}{X}$, $k \ne 0$. Moreover, if $|\hat{\omega}| \ge \frac{\pi}{X}$ we have $S_W(\hat{\omega}) = \frac{\alpha}{|\hat{\omega}|}$. This futher implies for $k \ge 1$ we will have $S_W(\hat{\omega}) \ge S_W(|\frac{2\pi k}{X}|)$ and for $k \le 1$ we will have $S_W(\hat{\omega}) \ge S_W(|\frac{2 \pi (K+1)}{X})$.

Thus we can equivalently write:
$$S_{W_s}(\omega) = \frac{S_W(\omega)}{X} + \frac{1}{X} \alpha \sum_{k=1}^{\infty} \frac{X}{2\pi k} + \frac{X}{2\pi (k+1)}$$
$$S_{W_s}(\omega) \ge \frac{S_W(\omega)}{X} + \frac{\alpha}{2\pi} \sum_{k=2}\frac{1}{k}$$

Clearly, $S_{W_s}(\omega)$ diverges for all $\omega$. Therefore $\sigma^2 = \int_{0}^{\infty}S_{W_s}(\omega)$ also diverges. Thus variance of the sampled noise is $\infty$.
% further, we know, power spectral density is always positive. Also the power spectral density of a real valued process is a real and even function of frequency. Therefore $S_W(\omega) = S_W(-\omega)$.

% This gives us:
% $$S_{W_s}(\omega) = \frac{S_W(\omega)}{X} +  \frac{1}{X} \sum_{k=1}^{\infty} S_W(\omega - \frac{2 \pi k}{X}) + \frac{1}{X} \sum_{k=-\infty}^{-1} S_W(\omega - \frac{2 \pi k}{X})$$
% Using the fact that $X$ is sufficiently small we get:
% $$\sigma^2 = \frac{S_W(0)}{X} +  \frac{2}{X} \sum_{k=1}^{\infty} \frac{\alpha X}{2 \pi k}$$
% $$\sigma^2 = \frac{S_W(0)}{X} + \frac{\alpha}{\pi} \sum_{k=1}^{\infty} \frac{1}{k}$$

% Clearly, this divergest to $\infty$ and therefore the variance of sampled white noise is $\infty$.

\section*{Q3}


\section*{Q4}
We are given a polynomial field $g(x) = a + bx + cx^2$ where $x \in [0, 1]$. Denote the legendre polynomial of degree k by $p_k(x)$. We know that the legendre polynomials are orthogonal in $[-1, 1]$ but the function $g$ given to  us is in $[0, 1]$. So we have to first convert it into another polynomial with range $[-1, 1]$.

We choose $h(x) = g(\frac{x+1}{2})$. Clearly the domain of definition for $h$ is $[-1, 1]$. Let $h(x) = \alpha + \beta x + \gamma x^2$ then we get:
$$\alpha = a + \frac{b}{2} + \frac{c}{2}$$
$$\beta = \frac{b}{2} + c$$
$$\gamma = \frac{c}{2}$$

Clearly, estimating $a, b, c$ is equivalent to estimating $\alpha, \beta, \gamma$. So we now focus on estimating the latter. Denote the coefficients of $h$ in the legendre polynomial basis be given by $A[k]$. We can therefore write:
$$A[k] = \frac{2k+1}{2} \int_{-1}^1 h(x) p_k(x) dx$$

We know approximate it using reimann sum (M point approximation):
$$A_R[k] = \frac{2k+1}{2} \sum_{i=1}^M h(\frac{2i}{M}-1) p_k(\frac{2i}{M} -1)$$
Here we have replaced $\frac{i}{M}$ with $\frac{2i}{M}-1$ which is the transformation from g to h ($g(x) = h(2x-1)$).

We note that we do not have samples at $\frac{2i}{M} -1$ rather at points $\hat{S}_i = 2 S_i - 1$. So we estimate it using
$$\hat{A}[k] = \frac{2k+1}{2} \sum_{i=1}^M h(\hat{S}_i) p_k(\frac{2i}{M} -1)$$

First case we don't consider noise. We want to estimate $A[k]$ from $\hat{A}[k]$ and we try to give a bound for the same. All thefollowing derivations are from the paper on location unaware mobile sensor by Animesh Kumar.

$$\mathbb{E}[|\hat{A}[k] - A[k]|^2] \le 2 \mathbb{E}[|\hat{A}[k] - A_R[k]|^2] + 2 \mathbb{E}[|A_R[k] - A[k]|^2]$$

Consider the first term of RHS:

$$|\hat{A}[k] - A_R[k]| = |\frac{1}{M}\sum_{i=1}^M[h(\hat{S_i}) - h(\frac{2i}{M} -1)]p_k(\frac{2i}{M} -1)|$$
$$|\hat{A}[k] - A_R[k]|^2 \le ||h||_{\infty}^2 \frac{1}{M}\sum_{i=1}^M |\hat{S_i} - (\frac{2i}{M} -1)|^2 p_k(\frac{2i}{M}-1)$$
$$|\hat{A}[k] - A_R[k]|^2 \le ||h||_{\infty}^2 \frac{1}{M}\sum_{i=1}^M |\hat{S_i} - (\frac{2i}{M} -1)|^2$$
The last step follows because $p_k(x)$ are bounded between [-1,1].

Following the result from the paper we get:
$$\mathbb{E}[|\hat{A}[k] - A_R[k]|^2] \le \frac{C_1}{n}$$
Here $n$ is the oversampling rate.

Now consider the second term of RHS:
$$|A_R[k] - A[k]| = |\frac{1}{M} \sum_{i=1}^M [h(\frac{2i}{M} -1)p_k(\frac{2i}{M} -1) - \int_{\frac{2i}{M} -1)}^{\frac{2i+2}{M}-1}h(x)p_k(x)dx]$$
For some constants $Z_{i,m} \in [\frac{2i}{M} -1), \frac{2i+2}{M} -1)]$ we have:
$$|A_R[k] - A[k]| = |\frac{1}{M} \sum_{i=1}^M [h(\frac{2i}{M} -1)p_k(\frac{2i}{M} -1) - h(Z_{i,m})p_k(Z_{i,m})dx]$$
$$|A_R[k] - A[k]| \le \frac{1}{M}\sum_{i=1}^M |Z_{i,m} - \frac{i}{M}| ||\frac{d}{dx}h(x)p_k(x)||_{\infty}$$
$$|A_R[k] - A[k]| \le \frac{1}{M}\sum_{i=1}^M \frac{1}{M} ||\frac{d}{dx}h(x)p_k(x)||_{\infty}$$
$$|A_R[k] - A[k]| \le \frac{1}{M} ||\frac{d}{dx}h(x)p_k(x)||_{\infty}$$
$$|\frac{d}{dx}h(x)p_k(x)| = |h(x)p_k'(x) + h'(x)p_k(x)| \le |h(x)| + |h'(x)| \le C_2$$
$$|A_R[k] - A[k]| \le \frac{C_2}{M} \le \frac{C_3}{(n-\lambda)}$$
$$\mathbb{E}[|A_R[k] - A[k]|^2] \le \frac{C_4}{(n-\lambda)^2}$$

Therefore we get:
$$\mathbb{E}[|\hat{A}[k] - A[k]|^2] \le 2 \frac{C1}{n} + 2 \frac{C4}{(n-\lambda)^2}$$

So we see if number of samples are increased the error decreases as an order of $\frac{1}{n}$ and approaches 0. Once $A[k]$ are known we can estimate the original coefficients of $g$ as well.

For the case where additive noise exists, it will lead to an additional term corresponding to the averaged noise given by:
$$\mathbb{E}(|W_{avg}[k]|^2) = \mathbb{E}[|\frac{1}{M}\sum_{i=1}^M W(\hat{S}_i)p_k(\frac{2i}{M}-1)|^2]$$
$$\mathbb{E}(|W_{avg}[k]|^2) = \mathbb{E}[\frac{1}{M^2}\sum_{i=1}^M |W(\hat{S}_i)|^2]$$
$$\mathbb{E}(|W_{avg}[k]|^2) = \mathbb{E}[\frac{\sigma^2}{M}] \le \frac{C_5}{n-\lambda}$$

Even when this term is added the error still decreases as an order of $\frac{1}{n}$ and hence our previous claims still holds.
\end{document}
