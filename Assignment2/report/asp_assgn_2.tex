\documentclass{article}
\usepackage[a4paper, tmargin=1in, bmargin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}

% \usepackage{parskip}
\usepackage{amsmath}
\usepackage{siunitx}
\sisetup{round-mode=places, round-precision=4}
\usepackage{ bbold }

\usepackage{pdflscape}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\title{EE 771 : Recent Topics in Analytical Signal Processing Assignment 2}
\author{Arka Sadhu - 140070011}
\date{\today}

\begin{document}
\maketitle

\section*{Q1}
We are given the bandlimited field $g(x, y)$ as
$$g(x,y) = \sum_{l=-10}^{10}\sum_{k=-5}^{5} a[k, l] exp(j2\pi kx + j2\pi ly)$$
We are also given that we move along the path $y = \sqrt{2}x$ and this path is denoted by L. $g$ is parametrized by time as
\begin{align}
  \label{eq:1}
  h(t) = g(t, \sqrt(2)t) \tag*{$0 \le t \le 1/\sqrt{2}$}
\end{align}

Let the samples taken along the path L be separated by distances of $\Delta$. Slope of the line is $\sqrt{2}$ and thus its projections of x-axis and y-axis are $\Delta_x = \Delta / \sqrt{3} $ and $\Delta_y = \Delta \sqrt{2} / \sqrt{3}$

\subsection*{1a}
We need to find the degrees of freedom along the two axes of the 2d field $g$. First we consider along the x-axis i.e. y is a constant (here taken to be $y_0$) and x is variable.

$$g(x, y_0) = \sum_{l=-10}^{10} \sum_{k=-5}^5a[k, l] exp(j2 \pi kx + j2\pi ly_0)$$
$$g(x, y_0) = \sum_{k=-5}^{5} exp(j2 \pi kx) \sum_{l=-10}^{10} a[k, l] exp(j2\pi ly_0)$$
Let $\alpha [k] = \sum_{l=-10}^{10}a[k, l] exp(j2\pi ly_0)$
\begin{equation}
  \label{eq:gy0}
  g(x, y_0) = \sum_{k=-5}^{5} exp(j2 \pi kx) \alpha[k]
\end{equation}

Therefore we can reconstruct $g(x, y_0)$ from the values of $\alpha[k]$ and 11 such values are required. Therefore degrees of freedom along x-axis is 11.

For y-axis we have constant x (say $x_0$) and variable y.
$$\beta[l] = \sum_{k=-5}^5 a[k,l] exp(j2\pi kx_0)$$
\begin{equation}
  \label{eq:gx0}
  g(x_0, y) = \sum_{l=-10}^{10}exp(j2 \pi ly) \beta[l]
\end{equation}

Again, we can reconstruct $g(x_0, y)$ from the values of $\beta[l]$ and 21 such values are required. Therefore degrees of freedom along y-axis is 21.

\subsection*{1b}
We note that the representations in \ref{eq:gy0} and \ref{eq:gx0} readily suggest the bandlimitness of the 1d representation. For the case of $g(x, 0.5)$ we note that \ref{eq:gy0} directly implies that the maximum frequency requried would be when $k = +5$ or $k = -5$ both of which correspond to 5Hz. Also we note that this value is independent of the value of $y_0$. Similarly for the case of $g(0.25, y)$ we have maximum frequency of 10Hz. Therefore both 1d representations are bandlimited.

\subsection*{1c}
We first note that $g(x,y)$ is bandlimited to $[-\rho_x, \rho_x] x [-\rho_y, \rho_y]$ where $\rho_x = 10\pi$ and $\rho_y = 20 \pi$. For $h(t)$ to cover all degrees of freedom of $g(x, y)$ we need that there should be no aliasing in either dimension. That is, we need $\Delta_x \le \pi / \rho_x$ and $\Delta_y \le \pi / \rho_y$. That is:
$$\Delta / \sqrt{3} \le 1 /10$$
$$\Delta \sqrt{2} / \sqrt{3} \le 1 / 20$$
The above reduces to $\Delta \le \sqrt{3} / 10$ and $\Delta \le \sqrt{3}/(20 \sqrt{2})$. Clearly the second inequality is stronger and therefore we need $\Delta \le \sqrt{3} / (20 * \sqrt{2})$ so that $h(t)$ can capture all degrees of freedom of $g(x, y)$.

\subsection*{1d}
As shown in above part, if $\Delta \le \sqrt{3} / (20 / \sqrt{2})$ then we can reconstruct $g(x, y)$ from $h(t)$.

\section*{Q2}
We are given that $W(x)$ is a white noise process. We are also given that $S_W(\omega) \propto \frac{1}{\omega}$ for $|\omega| \ge \frac{\pi}{X}$ where $X$ is the sampling distance.

From power spectral density theory we know that:
\begin{align}
  \label{eq:psd1}
  S_{W_s}(\omega) = \frac{1}{X}\sum_{k=-\infty}^{\infty}S_W(\omega - \frac{2\pi k}{X}) \tag*{$|\omega| \le \frac{\pi}{X}$}
\end{align}
We note that the required variance of $W_S$ is given by $\sigma^2 = S_{W_s}(0)$
$$S_{W_s}(0) = \frac{1}{X} \sum_{k=-\infty}^{\infty}S_W(\frac{2\pi k}{X})$$
$$\sigma^2 = S_{W_s}(0) = \frac{S_W(0)}{X} + \frac{1}{X} \sum_{k \in \mathcal{Z}, k \ne 0} S_W(\frac{2 \pi k}{X})$$

Further, we know, power spectral density is always positive. Also the power spectral density of a real valued process is a real and even function of frequency. Therefore $S_W(\omega) = S_W(-\omega)$.

This gives us:
$$\sigma^2 = \frac{S_W(0)}{X} +  \frac{2}{X} \sum_{k=1}^{\infty} S_W(\frac{2 \pi k}{X})$$
Using the fact that $X$ is sufficiently small we get:
$$\sigma^2 = \frac{S_W(0)}{X} +  \frac{2}{X} \sum_{k=1}^{\infty} \frac{\alpha X}{2 \pi k}$$
$$\sigma^2 = \frac{S_W(0)}{X} + \frac{\alpha}{\pi} \sum_{k=1}^{\infty} \frac{1}{k}$$

Clearly, this divergest to $\infty$ and therefore the variance of sampled white noise is $\infty$.

\section*{Q3}

\section*{Q4}
We are given a polynomial field $g(x) = a + bx + cx^2$ where $x \in [0, 1]$. Denote the legendre polynomial of degree k by $p_k(x)$. We know that the legendre polynomials are orthogonal in $[-1, 1]$ but the function $g$ given to  us is in $[0, 1]$. So we have to first convert it into another polynomial with range $[-1, 1]$.

We choose $h(x) = g(\frac{x+1}{2})$. Clearly the domain of definition for $h$ is $[-1, 1]$. Let $h(x) = \alpha + \beta x + \gamma x^2$ then we get:
$$\alpha = a + \frac{b}{2} + \frac{c}{2}$$
$$\beta = \frac{b}{2} + c$$
$$\gamma = \frac{c}{2}$$

Clearly, estimating $a, b, c$ is equivalent to estimating $\alpha, \beta, \gamma$. So we now focus on estimating the latter. Denote the coefficients of $h$ in the legendre polynomial basis be given by $A[k]$. We can therefore write:
$$A[k] = \frac{2k+1}{2} \int_{-1}^1 h(x) p_k(x) dx$$

We know approximate it using reimann sum (M point approximation):
$$A_R[k] = \frac{2k+1}{2} \sum_{i=1}^M h(\frac{2i}{M}-1) p_k(\frac{2i}{M} -1)$$
Here we have replaced $\frac{i}{M}$ with $\frac{2i}{M}-1$ which is the transformation from g to h ($g(x) = h(2x-1)$).

We note that we do not have samples at $\frac{2i}{M} -1$ rather at points $\hat{S}_i = 2 S_i - 1$. So we estimate it using
$$\hat{A}[k] = \frac{2k+1}{2} \sum_{i=1}^M h(\hat{S}_i) p_k(\frac{2i}{M} -1)$$

First case we don't consider noise. We want to estimate $A[k]$ from $\hat{A}[k]$ and we try to give a bound for the same. All thefollowing derivations are from the paper on location unaware mobile sensor by Animesh Kumar.

$$\mathbb{E}[|\hat{A}[k] - A[k]|^2] \le 2 \mathbb{E}[|\hat{A}[k] - A_R[k]|^2] + 2 \mathbb{E}[|A_R[k] - A[k]|^2]$$

Consider the first term of RHS:

$$|\hat{A}[k] - A_R[k]| = |\frac{1}{M}\sum_{i=1}^M[h(\hat{S_i}) - h(\frac{2i}{M} -1)]p_k(\frac{2i}{M} -1)|$$
$$|\hat{A}[k] - A_R[k]|^2 \le ||h||_{\infty}^2 \frac{1}{M}\sum_{i=1}^M |\hat{S_i} - (\frac{2i}{M} -1)|^2 p_k(\frac{2i}{M}-1)$$
$$|\hat{A}[k] - A_R[k]|^2 \le ||h||_{\infty}^2 \frac{1}{M}\sum_{i=1}^M |\hat{S_i} - (\frac{2i}{M} -1)|^2$$
The last step follows because $p_k(x)$ are bounded between [-1,1].

Following the result from the paper we get:
$$\mathbb{E}[|\hat{A}[k] - A_R[k]|^2] \le \frac{C_1}{n}$$
Here $n$ is the oversampling rate.

Now consider the second term of RHS:
$$|A_R[k] - A[k]| = |\frac{1}{M} \sum_{i=1}^M [h(\frac{2i}{M} -1)p_k(\frac{2i}{M} -1) - \int_{\frac{2i}{M} -1)}^{\frac{2i+2}{M}-1}h(x)p_k(x)dx]$$
For some constants $Z_{i,m} \in [\frac{2i}{M} -1), \frac{2i+2}{M} -1)]$ we have:
$$|A_R[k] - A[k]| = |\frac{1}{M} \sum_{i=1}^M [h(\frac{2i}{M} -1)p_k(\frac{2i}{M} -1) - h(Z_{i,m})p_k(Z_{i,m})dx]$$
$$|A_R[k] - A[k]| \le \frac{1}{M}\sum_{i=1}^M |Z_{i,m} - \frac{i}{M}| ||\frac{d}{dx}h(x)p_k(x)||_{\infty}$$
$$|A_R[k] - A[k]| \le \frac{1}{M}\sum_{i=1}^M \frac{1}{M} ||\frac{d}{dx}h(x)p_k(x)||_{\infty}$$
$$|A_R[k] - A[k]| \le \frac{1}{M} ||\frac{d}{dx}h(x)p_k(x)||_{\infty}$$
$$|\frac{d}{dx}h(x)p_k(x)| = |h(x)p_k'(x) + h'(x)p_k(x)| \le |h(x)| + |h'(x)| \le C_2$$
$$|A_R[k] - A[k]| \le \frac{C_2}{M} \le \frac{C_3}{(n-\lambda)}$$
$$\mathbb{E}[|A_R[k] - A[k]|^2] \le \frac{C_4}{(n-\lambda)^2}$$

Therefore we get:
$$\mathbb{E}[|\hat{A}[k] - A[k]|^2] \le 2 \frac{C1}{n} + 2 \frac{C4}{(n-\lambda)^2}$$

So we see if number of samples are increased the error decreases as an order of $\frac{1}{n}$ and approaches 0. Once $A[k]$ are known we can estimate the original coefficients of $g$ as well.

For the case where additive noise exists, it will lead to an additional term corresponding to the averaged noise given by:
$$\mathbb{E}(|W_{avg}[k]|^2) = \mathbb{E}[|\frac{1}{M}\sum_{i=1}^M W(\hat{S}_i)p_k(\frac{2i}{M}-1)|^2]$$
$$\mathbb{E}(|W_{avg}[k]|^2) = \mathbb{E}[\frac{1}{M^2}\sum_{i=1}^M |W(\hat{S}_i)|^2]$$
$$\mathbb{E}(|W_{avg}[k]|^2) = \mathbb{E}[\frac{\sigma^2}{M}] \le \frac{C_5}{n-\lambda}$$

Even when this term is added the error still decreases as an order of $\frac{1}{n}$ and hence our previous claims still holds.
\end{document}
